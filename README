This code provides a matrix-vector multiplication server using PETSc and client programs 
for Octave or Matlab. It is useful for carrying out 
matrix-vector multiplications with large matrices, the building blocks of virtually all 
iterative algorithms, in parallel inside of Octave or Matlab (or in a 
standalone program). For example, one can program an algorithm with Matlab or 
Octave utilizing very large matrices, by doing matrix-vector multiplications 
on a cluster in parallel, while taking advantage of built in operations for vectors 
and the simplicity of developing in a simple language. The matrix-vector ops are performed 
via overloaded operators and look like regular mat-vec calls (i.e. A*x and At*x). 
This software has been used to program iterative algorithms in Octave, using matrices 
upwards of 1 TB in size.

Written by Sergey Voronin
Last updated: September 2014

====== petsc installation ========

PETSc (http://www.mcs.anl.gov/petsc/) must be installed and correctly 
sourced prior to use.

basic installation (assume 64 indices for large matrix support)

mkdir /opt/petsc_software/

wget and untar petsc-3.5.1.tar.gz (or latest version)
cd petsc-3.5.1/

./configure --with-cc=gcc --with-cxx=g++ --with-fc=0 --download-f2cblaslapack --download-mpich --with-64-bit-indices

make PETSC_DIR=/opt/petsc_software/petsc-3.5.1 PETSC_ARCH=arch-linux2-c-debug all

make PETSC_DIR=/opt/petsc_software/petsc-3.5.1 PETSC_ARCH=arch-linux2-c-debug test

====== code compilation ========

First, make sure paths in setup_vars.sh are correct for your system and installations.

run the script compile_all.sh

this compiles the c code in server_code/ and client_code/ folders 
as well as the code to make the test matrix in PETSc binary format in make_test_matrix/  


====== usage ========

First, make a matrix in PETSc binary format. For an example of how to do this, 
one can use the program
make_test_matrix/write_diagonal_matrix.c
by executing the write_diagonal_matrix executable. This makes a 1000x1000 
(or differently sized) diagonal matrix D = diag(2,2,...,2). Now we can use the 
server with the matrix make_test_matrix/diagonalMatrix1.petsc. In reality, one 
can use a very large matrix, limited only by ram availability.

The server must be launched, this maybe done using the supplied start_server script. 
Modify the script to use the right number of MPI processes for your system. Note 
that the script must setup the correct paths to PETSc and MPI.

./start_server.sh

This loads the matrix in memory and awaits commands. When a multiply command is received, 
the server multiplies (in parallel) the matrix or its transpose by the given vector and 
writes the result to disk to be read in by a Matlab or Octave client.

Then in a different terminal if testing with the client and server on the same machine 
(or if on a cluster, on a single core of a node), open Octave or Matlab 
and execute compute_Ax_and_Atx.m . This is an example of using the mat-vec server. It will compare 
the results of doing the mat-vec operations with the matrix we generated above multiplied via 
the server program using PETSc to those obtained via computation in Matlab or Octave.

octave:3> compute_Ax_and_Atx
do y=A*x  
writing x1 to /petsc_server_test_new1/petsc_data1//x1.petsc
ls: cannot access /petsc_server_test_new1/petsc_data1//y1.pets
c: No such file or directory
waiting for output
reading y1 from /petsc_server_test_new1/petsc_data1//y1.petsc
Variables in the current scope:

   Attr Name        Size                     Bytes  Class
   ==== ====        ====                     =====  =====
        x        1000x1                       8000  double
        y        1000x1                       8000  double

Total is 2000 elements using 16000 bytes

done with mult: norm(x) = 17.697000, norm(y) = 35.393999
perror = 0.000000
do yt = At*xt
writing x1 to /petsc_server_test_new1/petsc_data1//x1.petsc
ls: cannot access /petsc_server_test_new1/petsc_data1//y1.petsc: No such file or directory
waiting for output
reading y1 from /petsc_server_test_new1/petsc_data1//y1.petsc
Variables in the current scope:

   Attr Name        Size                     Bytes  Class
   ==== ====        ====                     =====  =====
        xt       1000x1                       8000  double
        yt       1000x1                       8000  double

Total is 2000 elements using 16000 bytes

done with mult: norm(xt) = 18.731915, norm(yt) = 37.463829
perror = 0.000000
octave:4> 

